{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41a5aa46",
   "metadata": {},
   "source": [
    "# Scénario 2 : Analyse Linguistique d'un Corpus de Transcriptions Orales\n",
    "\n",
    "**Chercheur (fictif) :** Jean Dupont, sociolinguiste.\n",
    "\n",
    "**Phase du projet :** Milieu de recherche. Il a déjà collecté et transcrit un corpus d'entretiens oraux.\n",
    "\n",
    "**Objectif de cet accompagnement (simulé via ce notebook) :**\n",
    "1. Nettoyer et préparer ce corpus textuel pour une analyse outillée.\n",
    "2. Identifier les tours de parole pour chaque locuteur.\n",
    "3. Appliquer des techniques de Traitement Automatique de la Langue (TAL) pour enrichir le texte (lemmatisation, étiquetage grammatical, reconnaissance d'entités).\n",
    "4. Obtenir des premières analyses quantitatives sur le corpus (fréquences de mots, types de mots utilisés) pour identifier des pistes de variations linguistiques.\n",
    "\n",
    "**Corpus utilisé :** Trois fichiers de transcriptions (`entretien_A.txt`, `entretien_B.txt`, `entretien_C.txt`) situés dans le dossier `corpus_exemple/`.\n",
    "\n",
    "---\n",
    "**Méthodologie :**\n",
    "* Lecture des fichiers texte.\n",
    "* Segmentation des tours de parole par locuteur et nettoyage des annotations d'oralité.\n",
    "* Utilisation de la bibliothèque `spaCy` et de son modèle français (`fr_core_news_md`) pour l'analyse NLP.\n",
    "* Analyses quantitatives (fréquences, distributions) et visualisations (`matplotlib`, `wordcloud`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8edb62",
   "metadata": {},
   "source": [
    "## 0. Importation des bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523c1592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # Pour des graphiques un peu plus esthétiques\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Configuration de l'affichage\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a771001",
   "metadata": {},
   "source": [
    "## 1. Chargement et Préparation des Données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db662f4e",
   "metadata": {},
   "source": [
    "### 1.1. Définition des Chemins et Création des Dossiers de Sortie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db571fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_DIR = 'corpus_exemple/' \n",
    "OUTPUT_DIR = 'output_data/'\n",
    "PLOTS_DIR = os.path.join(OUTPUT_DIR, 'plots')\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "if not os.path.exists(PLOTS_DIR):\n",
    "    os.makedirs(PLOTS_DIR)\n",
    "\n",
    "print(f\"Dossier du corpus : {os.path.abspath(CORPUS_DIR)}\")\n",
    "print(f\"Dossier de sortie : {os.path.abspath(OUTPUT_DIR)}\")\n",
    "print(f\"Dossier des graphiques : {os.path.abspath(PLOTS_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5c1848",
   "metadata": {},
   "source": [
    "### 1.2. Lecture des Fichiers de Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f59bccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transcriptions(directory_path):\n",
    "    transcriptions = {}\n",
    "    try:\n",
    "        for filename in os.listdir(directory_path):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(directory_path, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    transcriptions[filename] = f.read()\n",
    "        print(f\"{len(transcriptions)} fichiers de transcription chargés depuis '{directory_path}'.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERREUR: Le dossier '{directory_path}' n'a pas été trouvé. Vérifiez le chemin.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur est survenue lors du chargement des fichiers: {e}\")\n",
    "        return None\n",
    "    return transcriptions\n",
    "\n",
    "raw_transcriptions = load_transcriptions(CORPUS_DIR)\n",
    "\n",
    "if raw_transcriptions and 'entretien_A.txt' in raw_transcriptions:\n",
    "    print(\"\\nExtrait de 'entretien_A.txt' (premiers 500 caractères) :\")\n",
    "    print(raw_transcriptions['entretien_A.txt'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddcf5cd",
   "metadata": {},
   "source": [
    "### 1.3. Nettoyage Initial et Segmentation par Locuteur\n",
    "\n",
    "Nous allons maintenant :\n",
    "1.  Identifier les tours de parole et le locuteur associé.\n",
    "2.  Nettoyer le texte de chaque tour de parole en enlevant les annotations d'oralité et les marques de locuteur du texte lui-même."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ab30d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_segment_transcriptions(transcriptions_dict):\n",
    "    all_turns_data = []\n",
    "    locutor_pattern = re.compile(r\"^([A-ZÉÈÀÊÂÄÇÏÎÔÖÛÜŸŒÆ][A-ZÉÈÀÊÂÄÇÏÎÔÖÛÜŸŒÆ\\s()'-]+):(.*)\")\n",
    "    annotation_pattern = re.compile(r\"\\([^)]+\\)\") \n",
    "    square_bracket_pattern = re.compile(r\"\\[[^\\]]+\\]\") \n",
    "\n",
    "    if not transcriptions_dict:\n",
    "        print(\"Aucune transcription à traiter.\")\n",
    "        return []\n",
    "\n",
    "    for doc_id, content in transcriptions_dict.items():\n",
    "        lines = content.splitlines()\n",
    "        current_locutor = None\n",
    "        current_speech_parts = []\n",
    "\n",
    "        for line in lines:\n",
    "            line_stripped = line.strip()\n",
    "            if not line_stripped: \n",
    "                continue\n",
    "\n",
    "            match = locutor_pattern.match(line_stripped) # Match on stripped line to avoid leading spaces issues\n",
    "            if match:\n",
    "                if current_locutor and current_speech_parts:\n",
    "                    raw_speech = \" \".join(current_speech_parts).strip()\n",
    "                    all_turns_data.append({\n",
    "                        'doc_id': doc_id,\n",
    "                        'locutor': current_locutor,\n",
    "                        'raw_speech': raw_speech\n",
    "                    })\n",
    "                \n",
    "                current_locutor = match.group(1).strip()\n",
    "                current_locutor = current_locutor.split('(')[0].strip() # Normalize locutor name\n",
    "                current_speech_parts = [match.group(2).strip()]\n",
    "            elif current_locutor: \n",
    "                current_speech_parts.append(line_stripped) # Append stripped line\n",
    "        \n",
    "        if current_locutor and current_speech_parts:\n",
    "            raw_speech = \" \".join(current_speech_parts).strip()\n",
    "            all_turns_data.append({\n",
    "                'doc_id': doc_id,\n",
    "                'locutor': current_locutor,\n",
    "                'raw_speech': raw_speech\n",
    "            })\n",
    "\n",
    "    for turn in all_turns_data:\n",
    "        text = turn['raw_speech']\n",
    "        text = annotation_pattern.sub('', text) \n",
    "        text = square_bracket_pattern.sub('', text) \n",
    "        text = text.replace('euh...', 'euh ').replace('hum...', 'hum ') # Add space after normalized hesitation\n",
    "        text = text.replace('...', ' ') \n",
    "        text = text.lower() \n",
    "        text = re.sub(r'\\s+', ' ', text).strip() \n",
    "        turn['cleaned_speech'] = text\n",
    "        \n",
    "    return all_turns_data\n",
    "\n",
    "if raw_transcriptions:\n",
    "    segmented_data = preprocess_and_segment_transcriptions(raw_transcriptions)\n",
    "    df_turns = pd.DataFrame(segmented_data)\n",
    "    print(f\"\\n{len(df_turns)} tours de parole extraits.\")\n",
    "    if not df_turns.empty:\n",
    "        print(\"Aperçu des tours de parole :\")\n",
    "        display(df_turns.head())\n",
    "        print(\"\\nLocuteurs uniques identifiés :\")\n",
    "        display(df_turns['locutor'].value_counts())\n",
    "        df_turns.info()\n",
    "    else:\n",
    "        print(\"Aucun tour de parole n'a pu être extrait.\")\n",
    "else:\n",
    "    print(\"Pas de transcriptions chargées, la segmentation ne peut pas être effectuée.\")\n",
    "    df_turns = pd.DataFrame() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eb9e4c",
   "metadata": {},
   "source": [
    "## 2. (Note) Discussion sur la Structuration Avancée (XML-TEI)\n",
    "\n",
    "Pour des corpus oraux destinés à une analyse approfondie, à la pérennisation et au partage dans la communauté scientifique, le format XML-TEI (Text Encoding Initiative) est souvent recommandé. Il permet une description très riche de la structure du texte, des locuteurs, des phénomènes prosodiques, des annotations, etc. \n",
    "\n",
    "Par exemple, un tour de parole pourrait être balisé ainsi :\n",
    "```xml\n",
    "<u who=\"#LOC_MARC\">\n",
    "  <seg>Ah, salut Chloé ! Oui, justement, j'en sors à l'instant, enfin... virtuellement, quoi. <incident><desc>rire</desc></incident> J'ai passé la matinée à essayer de... de dompter les outils d'OCR sur des vieux numéros du <title rend=\"italic\">Petit Journal</title>.</seg>\n",
    "</u>\n",
    "```\n",
    "Cette structuration fine facilite des requêtes complexes et l'interopérabilité avec d'autres outils et corpus. Pour cette démonstration, nous nous en tenons à une structure tabulaire simple (DataFrame Pandas), mais il est bon de connaître l'existence et les avantages de la TEI pour des projets de plus grande envergure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcefe08",
   "metadata": {},
   "source": [
    "## 3. Traitement Automatique de la Langue (TAL) avec spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0a6698",
   "metadata": {},
   "source": [
    "### 3.1. Chargement du Modèle spaCy Français\n",
    "\n",
    "Nous allons utiliser le modèle `fr_core_news_md` qui offre un bon compromis entre performance et taille. Si vous ne l'avez pas, vous pouvez l'installer via : `python -m spacy download fr_core_news_md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f57eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nlp = spacy.load('fr_core_news_md')\n",
    "    print(\"Modèle spaCy 'fr_core_news_md' chargé avec succès.\")\n",
    "except OSError:\n",
    "    print(\"Modèle spaCy 'fr_core_news_md' non trouvé. Veuillez l'installer avec la commande :\")\n",
    "    print(\"python -m spacy download fr_core_news_md\")\n",
    "    print(\"Pour cette démo, les étapes NLP seront sautées si le modèle n'est pas chargé.\")\n",
    "    nlp = None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182ed71f",
   "metadata": {},
   "source": [
    "### 3.2. Application des Traitements NLP aux Tours de Parole\n",
    "\n",
    "Nous allons maintenant appliquer le pipeline NLP de spaCy (tokenisation, lemmatisation, POS-tagging, reconnaissance d'entités nommées) à chaque tour de parole nettoyé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85153c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_nlp_features(text, nlp_pipeline):\n",
    "    if pd.isna(text) or not nlp_pipeline or not text.strip(): # Ajout de not text.strip() pour les chaînes vides\n",
    "        return {'tokens': [], 'lemmas': [], 'pos_tags': [], 'entities': [], 'nlp_doc': None}\n",
    "    \n",
    "    doc = nlp_pipeline(text)\n",
    "    \n",
    "    tokens = [token.text for token in doc]\n",
    "    lemmas = [token.lemma_ for token in doc if not token.is_punct and not token.is_space and not token.is_stop]\n",
    "    pos_tags = [token.pos_ for token in doc if not token.is_punct and not token.is_space and not token.is_stop]\n",
    "    entities = [{'text': ent.text, 'label': ent.label_} for ent in doc.ents]\n",
    "    \n",
    "    return {'tokens': tokens, 'lemmas': lemmas, 'pos_tags': pos_tags, 'entities': entities, 'nlp_doc': doc}\n",
    "\n",
    "if nlp and not df_turns.empty and 'cleaned_speech' in df_turns.columns:\n",
    "    print(\"Application des traitements NLP en cours (cela peut prendre un moment)...\")\n",
    "    # S'assurer que cleaned_speech ne contient pas de NaN pour apply\n",
    "    df_turns['cleaned_speech'] = df_turns['cleaned_speech'].fillna('')\n",
    "    \n",
    "    nlp_results = df_turns['cleaned_speech'].apply(lambda x: apply_nlp_features(x, nlp))\n",
    "    \n",
    "    df_turns['tokens'] = nlp_results.apply(lambda x: x['tokens'])\n",
    "    df_turns['lemmas'] = nlp_results.apply(lambda x: x['lemmas'])\n",
    "    df_turns['pos_tags'] = nlp_results.apply(lambda x: x['pos_tags'])\n",
    "    df_turns['entities'] = nlp_results.apply(lambda x: x['entities'])\n",
    "    # df_turns['nlp_doc'] = nlp_results.apply(lambda x: x['nlp_doc']) # Optionnel, peut rendre le DF lourd\n",
    "    df_turns['num_lemmas'] = df_turns['lemmas'].apply(len)\n",
    "    \n",
    "    print(\"\\nTraitements NLP terminés.\")\n",
    "    print(\"Aperçu du DataFrame enrichi :\")\n",
    "    display(df_turns[['doc_id', 'locutor', 'cleaned_speech', 'num_lemmas', 'lemmas', 'pos_tags', 'entities']].head())\n",
    "else:\n",
    "    print(\"Le modèle NLP n'est pas chargé ou le DataFrame df_turns est vide ou la colonne 'cleaned_speech' est manquante. Les fonctionnalités NLP ne peuvent pas être appliquées.\")\n",
    "    if not df_turns.empty:\n",
    "        for col in ['tokens', 'lemmas', 'pos_tags', 'entities']:\n",
    "             if col not in df_turns.columns:\n",
    "                df_turns[col] = [[] for _ in range(len(df_turns))] # Initialiser avec des listes vides\n",
    "        if 'num_lemmas' not in df_turns.columns:\n",
    "             df_turns['num_lemmas'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eb56e7",
   "metadata": {},
   "source": [
    "## 4. Analyse Quantitative et Exploration\n",
    "\n",
    "Maintenant que nos données sont enrichies, nous pouvons effectuer quelques analyses quantitatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e048d2",
   "metadata": {},
   "source": [
    "### 4.1. Statistiques Descriptives de Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb396fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_turns.empty and 'locutor' in df_turns.columns:\n",
    "    print(\"Statistiques descriptives de base :\")\n",
    "    print(f\"- Nombre total de documents analysés : {df_turns['doc_id'].nunique()}\")\n",
    "    print(f\"- Nombre total de tours de parole : {len(df_turns)}\")\n",
    "    print(f\"- Nombre de locuteurs uniques : {df_turns['locutor'].nunique()}\")\n",
    "\n",
    "    print(\"\\nNombre de tours de parole par locuteur :\")\n",
    "    display(df_turns['locutor'].value_counts())\n",
    "\n",
    "    if 'num_lemmas' in df_turns.columns and df_turns['num_lemmas'].notna().any() :\n",
    "        print(\"\\nStatistiques sur le nombre de lemmes (filtrés des stop-words et ponctuation) par tour de parole, par locuteur :\")\n",
    "        display(df_turns.groupby('locutor')['num_lemmas'].agg(['sum', 'mean', 'std', 'min', 'max']))\n",
    "        \n",
    "        # Calcul du Type-Token Ratio (TTR) sur les lemmes par locuteur (CORRECTED)\n",
    "        def calculate_ttr(lemmas_list_series): \n",
    "            if lemmas_list_series.empty: \n",
    "                return 0.0\n",
    "\n",
    "            all_lemmas_for_locutor = [\n",
    "                lemma \n",
    "                for sublist in lemmas_list_series \n",
    "                for lemma in sublist \n",
    "                if isinstance(sublist, list) \n",
    "            ]\n",
    "\n",
    "            if not all_lemmas_for_locutor: \n",
    "                return 0.0\n",
    "                \n",
    "            return len(set(all_lemmas_for_locutor)) / len(all_lemmas_for_locutor)\n",
    "\n",
    "        ttr_per_locutor = df_turns.groupby('locutor')['lemmas'].apply(calculate_ttr)\n",
    "        print(\"\\nType-Token Ratio (TTR) sur les lemmes par locuteur :\")\n",
    "        display(ttr_per_locutor)\n",
    "    else:\n",
    "        print(\"\\nColonne 'num_lemmas' non disponible ou vide, statistiques sur les lemmes non calculées.\")\n",
    "else:\n",
    "    print(\"DataFrame df_turns vide ou colonne 'locutor' manquante. Aucune statistique descriptive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25559b76",
   "metadata": {},
   "source": [
    "### 4.2. Analyse des Fréquences (Lemmes)\n",
    "\n",
    "Nous allons identifier les lemmes les plus fréquents globalement et par locuteur. Les lemmes ont été filtrés des mots vides (stop words) et de la ponctuation lors de l'étape NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f1ee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_lemmas(series_of_lemma_lists, n=15):\n",
    "    all_lemmas = [lemma for sublist in series_of_lemma_lists if isinstance(sublist, list) for lemma in sublist]\n",
    "    if not all_lemmas:\n",
    "        return pd.Series(dtype='int64')\n",
    "    return pd.Series(Counter(all_lemmas)).sort_values(ascending=False).head(n)\n",
    "\n",
    "if not df_turns.empty and 'lemmas' in df_turns.columns and df_turns['lemmas'].notna().any():\n",
    "    print(\"\\nTop 15 lemmes les plus fréquents (globalement) :\")\n",
    "    top_lemmas_global = get_top_n_lemmas(df_turns['lemmas'])\n",
    "    display(top_lemmas_global)\n",
    "    \n",
    "    print(\"\\nTop 10 lemmes les plus fréquents par locuteur :\")\n",
    "    for locutor in df_turns['locutor'].unique():\n",
    "        print(f\"  --- Locuteur : {locutor} ---\")\n",
    "        lemmas_locutor = df_turns[df_turns['locutor'] == locutor]['lemmas']\n",
    "        top_lemmas_locutor = get_top_n_lemmas(lemmas_locutor, n=10)\n",
    "        display(top_lemmas_locutor)\n",
    "else:\n",
    "    print(\"DataFrame vide ou colonne 'lemmas' manquante/vide. Analyse des fréquences de lemmes non effectuée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c05654e",
   "metadata": {},
   "source": [
    "### 4.3. Visualisation : Nuages de Mots par Locuteur\n",
    "\n",
    "Les nuages de mots donnent une représentation visuelle des termes les plus fréquents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4311cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if nlp and not df_turns.empty and 'lemmas' in df_turns.columns and df_turns['lemmas'].notna().any():\n",
    "    french_stop_words = list(nlp.Defaults.stop_words)\n",
    "    custom_stop_words = ['euh', 'ben', 'hein', 'quoi', 'oui', 'non', 'alors', 'être', 'avoir', 'faire', 'dire', 'pouvoir', 'aller', 'voir', 'vouloir', 'falloir', 'devoir', 'tout', 'ça', 'chose', 'truc', 'moment', 'personne', 'p’têt', 'pis', 'pis']\n",
    "    french_stop_words.extend(custom_stop_words)\n",
    "\n",
    "    for locutor in df_turns['locutor'].unique():\n",
    "        print(f\"\\nNuage de mots pour le locuteur : {locutor}\")\n",
    "        # Concatène les listes de lemmes pour ce locuteur\n",
    "        lemmas_locutor_list_of_lists = df_turns[df_turns['locutor'] == locutor]['lemmas'].tolist()\n",
    "        lemmas_locutor_flat_list = [lemma for sublist in lemmas_locutor_list_of_lists if isinstance(sublist, list) for lemma in sublist]\n",
    "        \n",
    "        filtered_lemmas_for_wc = [lemma for lemma in lemmas_locutor_flat_list if lemma not in french_stop_words and len(lemma) > 1]\n",
    "\n",
    "        if filtered_lemmas_for_wc:\n",
    "            text_for_wordcloud = \" \".join(filtered_lemmas_for_wc)\n",
    "            try:\n",
    "                wordcloud_obj = WordCloud(width=800, height=400, background_color='white', collocations=False, stopwords=french_stop_words).generate(text_for_wordcloud)\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.imshow(wordcloud_obj, interpolation='bilinear')\n",
    "                plt.axis('off')\n",
    "                plt.title(f'Nuage de mots pour {locutor}')\n",
    "                \n",
    "                locutor_filename_safe = re.sub(r'\\W+', '', locutor) \n",
    "                wc_path = os.path.join(PLOTS_DIR, f'wordcloud_{locutor_filename_safe}.png')\n",
    "                try:\n",
    "                   plt.savefig(wc_path)\n",
    "                   print(f\"Nuage de mots sauvegardé dans : {wc_path}\")\n",
    "                except Exception as e_save:\n",
    "                    print(f\"Erreur lors de la sauvegarde du nuage de mots pour {locutor}: {e_save}\")\n",
    "                plt.show()\n",
    "            except ValueError as ve:\n",
    "                 print(f\"  Impossible de générer le nuage de mots pour {locutor} (peut-être pas assez de mots après filtrage): {ve}\")   \n",
    "            except Exception as e_wc:\n",
    "                print(f\"  Erreur lors de la génération du nuage de mots pour {locutor}: {e_wc}\")\n",
    "        else:\n",
    "            print(f\"  Pas assez de lemmes significatifs pour générer un nuage de mots pour {locutor}.\")\n",
    "else:\n",
    "    print(\"NLP non chargé ou DataFrame vide/colonne 'lemmas' manquante. Nuages de mots non générés.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17d8a29",
   "metadata": {},
   "source": [
    "### 4.4. Analyse des Fréquences (Catégories Grammaticales - POS Tags)\n",
    "\n",
    "L'analyse de la distribution des POS tags peut donner des indications sur le style de parole (plus nominal, plus verbal, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b6e998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_pos(series_of_pos_lists, n=10):\n",
    "    all_pos = [pos for sublist in series_of_pos_lists if isinstance(sublist, list) for pos in sublist]\n",
    "    if not all_pos:\n",
    "        return pd.Series(dtype='int64')\n",
    "    return pd.Series(Counter(all_pos)).sort_values(ascending=False).head(n)\n",
    "\n",
    "pos_comparison_data = {} # Initialiser en dehors de la condition pour qu'elle existe toujours\n",
    "\n",
    "if not df_turns.empty and 'pos_tags' in df_turns.columns and df_turns['pos_tags'].notna().any():\n",
    "    print(\"\\nTop 10 POS tags les plus fréquents (globalement) :\")\n",
    "    top_pos_global = get_top_n_pos(df_turns['pos_tags'])\n",
    "    display(top_pos_global)\n",
    "    \n",
    "    print(\"\\nTop 5 POS tags les plus fréquents par locuteur :\")\n",
    "    for locutor in df_turns['locutor'].unique():\n",
    "        print(f\"  --- Locuteur : {locutor} ---\")\n",
    "        pos_locutor_series = df_turns[df_turns['locutor'] == locutor]['pos_tags']\n",
    "        top_pos_locutor = get_top_n_pos(pos_locutor_series, n=5)\n",
    "        display(top_pos_locutor)\n",
    "        \n",
    "        # Stocker pour graphique comparatif\n",
    "        flat_pos_list_locutor = [pos for sublist in pos_locutor_series if isinstance(sublist, list) for pos in sublist]\n",
    "        if flat_pos_list_locutor: # S'assurer qu'il y a des tags à compter\n",
    "            pos_counts_locutor = pd.Series(Counter(flat_pos_list_locutor))\n",
    "            if pos_counts_locutor.sum() > 0:\n",
    "                 pos_comparison_data[locutor] = pos_counts_locutor / pos_counts_locutor.sum()\n",
    "            else:\n",
    "                 pos_comparison_data[locutor] = pd.Series(dtype='float64') # Séries vides si pas de tags\n",
    "        else:\n",
    "            pos_comparison_data[locutor] = pd.Series(dtype='float64')\n",
    "else:\n",
    "    print(\"DataFrame vide ou colonne 'pos_tags' manquante/vide. Analyse des fréquences de POS non effectuée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2190b4",
   "metadata": {},
   "source": [
    "### 4.5. Visualisation : Distribution des POS Tags (Comparaison)\n",
    "\n",
    "Comparons les proportions des principales catégories grammaticales entre les locuteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41a7684",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'pos_comparison_data' in locals() and pos_comparison_data: \n",
    "    df_pos_comparison = pd.DataFrame(pos_comparison_data).fillna(0)\n",
    "    \n",
    "    if not df_pos_comparison.empty:\n",
    "        main_pos_tags = ['NOUN', 'VERB', 'ADJ', 'ADV', 'PROPN', 'PRON'] \n",
    "        # Filtrer pour ne garder que les lignes (POS tags) qui sont dans main_pos_tags ET présentes dans le DataFrame\n",
    "        relevant_pos_tags_in_df = [tag for tag in main_pos_tags if tag in df_pos_comparison.index]\n",
    "        df_pos_comparison_filtered = df_pos_comparison.loc[relevant_pos_tags_in_df]\n",
    "        \n",
    "        if not df_pos_comparison_filtered.empty:\n",
    "            df_pos_comparison_filtered.T.plot(kind='bar', figsize=(15, 7), colormap='viridis')\n",
    "            plt.title('Comparaison de la Distribution des Principaux POS Tags par Locuteur (Proportions)', fontsize=15)\n",
    "            plt.ylabel('Proportion', fontsize=12)\n",
    "            plt.xlabel('Locuteur', fontsize=12)\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.legend(title='POS Tag', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            pos_plot_path = os.path.join(PLOTS_DIR, 'distribution_pos_comparison.png')\n",
    "            try:\n",
    "                plt.savefig(pos_plot_path, bbox_inches='tight') # bbox_inches pour s'assurer que la légende est sauvegardée\n",
    "                print(f\"\\nGraphique de comparaison des POS tags sauvegardé dans : {pos_plot_path}\")\n",
    "            except Exception as e_save_pos:\n",
    "                print(f\"Erreur lors de la sauvegarde du graphique POS : {e_save_pos}\")\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Aucun des POS tags principaux sélectionnés ('NOUN', 'VERB', etc.) n'a été trouvé pour les locuteurs, ou les données sont insuffisantes.\")\n",
    "    else:\n",
    "        print(\"Le DataFrame de comparaison des POS est vide (aucun POS tag trouvé pour aucun locuteur).\")\n",
    "else:\n",
    "    print(\"Données de comparaison des POS non disponibles (variable 'pos_comparison_data' vide ou non définie).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dea74b",
   "metadata": {},
   "source": [
    "### 4.6. Analyse des Entités Nommées (NER)\n",
    "\n",
    "La Reconnaissance d'Entités Nommées (NER) identifie les mentions de personnes (PER), lieux (LOC), organisations (ORG), etc. Cela peut donner des indices sur les thèmes abordés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75176462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_entities(series_of_entity_lists, n=15, entity_types=None):\n",
    "    all_entities_text = []\n",
    "    for sublist_of_dicts in series_of_entity_lists:\n",
    "        if isinstance(sublist_of_dicts, list): # Assurer que c'est une liste\n",
    "            for entity_dict in sublist_of_dicts:\n",
    "                if isinstance(entity_dict, dict): # Assurer que c'est un dictionnaire\n",
    "                    if entity_types:\n",
    "                        if entity_dict.get('label') in entity_types:\n",
    "                            all_entities_text.append(entity_dict.get('text', '') + f\" ({entity_dict.get('label', '')})\")\n",
    "                    else:\n",
    "                         all_entities_text.append(entity_dict.get('text', '') + f\" ({entity_dict.get('label', '')})\")\n",
    "    if not all_entities_text:\n",
    "        return pd.Series(dtype='int64')\n",
    "    return pd.Series(Counter(all_entities_text)).sort_values(ascending=False).head(n)\n",
    "\n",
    "if not df_turns.empty and 'entities' in df_turns.columns and df_turns['entities'].notna().any():\n",
    "    print(\"\\nTop 15 Entités Nommées les plus fréquentes (globalement, tous types) :\")\n",
    "    top_entities_global = get_top_n_entities(df_turns['entities'])\n",
    "    display(top_entities_global)\n",
    "\n",
    "    print(\"\\nTop 5 Entités Nommées de type PER (Personne) par locuteur :\")\n",
    "    for locutor in df_turns['locutor'].unique():\n",
    "        print(f\"  --- Locuteur : {locutor} ---\")\n",
    "        entities_locutor = df_turns[df_turns['locutor'] == locutor]['entities']\n",
    "        top_pers_locutor = get_top_n_entities(entities_locutor, n=5, entity_types=['PER'])\n",
    "        if not top_pers_locutor.empty:\n",
    "            display(top_pers_locutor)\n",
    "        else:\n",
    "            print(\"    Aucune entité PER trouvée pour ce locuteur.\")\n",
    "    \n",
    "    all_entity_labels = []\n",
    "    for sublist_of_dicts in df_turns['entities']:\n",
    "        if isinstance(sublist_of_dicts, list):\n",
    "            for entity_dict in sublist_of_dicts:\n",
    "                if isinstance(entity_dict, dict):\n",
    "                    all_entity_labels.append(entity_dict.get('label','N/A'))\n",
    "    \n",
    "    if all_entity_labels:\n",
    "        df_entity_counts = pd.Series(Counter(all_entity_labels)).sort_values(ascending=False)\n",
    "        print(\"\\nDistribution globale des types d'entités nommées :\")\n",
    "        display(df_entity_counts)\n",
    "        \n",
    "        if not df_entity_counts.empty:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            df_entity_counts.plot(kind='bar', color='lightcoral')\n",
    "            plt.title('Distribution Globale des Types d\\'Entités Nommées', fontsize=15)\n",
    "            plt.ylabel('Nombre d\\'occurrences', fontsize=12)\n",
    "            plt.xlabel('Type d\\'entité', fontsize=12)\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            ner_plot_path = os.path.join(PLOTS_DIR, 'distribution_ner_types.png')\n",
    "            try:\n",
    "                plt.savefig(ner_plot_path)\n",
    "                print(f\"Graphique de distribution NER sauvegardé dans : {ner_plot_path}\")\n",
    "            except Exception as e_save_ner:\n",
    "                 print(f\"Erreur lors de la sauvegarde du graphique NER : {e_save_ner}\")\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"Aucune étiquette d'entité nommée trouvée dans le corpus pour le graphique.\")\n",
    "\n",
    "else:\n",
    "    print(\"DataFrame vide ou colonne 'entities' manquante/vide. Analyse des entités nommées non effectuée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ac95ad",
   "metadata": {},
   "source": [
    "### 4.7. Interprétation Initiale et Discussion (pour Jean Dupont)\n",
    "\n",
    "À ce stade, nous pouvons commencer à formuler des hypothèses pour Jean Dupont basées sur les observations :\n",
    "\n",
    "* **Richesse lexicale (TTR) :** Des différences de TTR entre locuteurs pourraient indiquer des styles de parole plus ou moins variés ou répétitifs.\n",
    "* **Lemmes fréquents :** Les lemmes spécifiques à certains locuteurs ou partagés peuvent révéler des thèmes de prédilection ou des tics de langage. Les termes techniques liés aux humanités numériques (`ocr`, `datalab`, `nakala`, `tei`, `gallica`, `huma-num`, `isidore`, `dariah`, `clarin`, `iiif`) sont très présents, ce qui est attendu vu le contenu des entretiens.\n",
    "* **Distribution des POS Tags :** Une prédominance de NOMS peut indiquer un style plus descriptif ou informatif, tandis qu'une abondance de VERBES peut signaler un discours plus axé sur l'action. Des différences entre locuteurs peuvent être significatives.\n",
    "* **Entités Nommées :** Les types d'entités (PER, LOC, ORG) et leur fréquence peuvent aider à cerner les sujets principaux abordés par chaque locuteur ou dans chaque entretien (ex: discussion centrée sur des personnes comme `Chloé`, `Marc`, `Mme Morel`, ou des institutions comme `BnF`, `Huma-Num`, `DataLab`).\n",
    "\n",
    "Ces analyses quantitatives sont une première étape. Elles doivent être complétées par une lecture qualitative et une contextualisation par le chercheur pour confirmer ou infirmer les pistes dégagées."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b895b620",
   "metadata": {},
   "source": [
    "## 5. Sauvegarde des Résultats Enrichis\n",
    "\n",
    "Nous sauvegardons le DataFrame principal contenant les tours de parole, les textes nettoyés et toutes les caractéristiques NLP extraites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30e8ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_turns.empty:\n",
    "    df_to_save = df_turns.copy()\n",
    "    for col in ['tokens', 'lemmas', 'pos_tags', 'entities']:\n",
    "        if col in df_to_save.columns:\n",
    "            # Convertir les listes/listes de dicts en chaînes pour CSV\n",
    "            def smart_join(data_list):\n",
    "                if not isinstance(data_list, list):\n",
    "                    return str(data_list) # Gérer les cas où ce n'est pas une liste (ex: NaN après un échec NLP partiel)\n",
    "                if not data_list: # Liste vide\n",
    "                    return \"\"\n",
    "                if isinstance(data_list[0], dict): # Pour la colonne 'entities'\n",
    "                    return \"; \".join([f\"{d.get('text','')}({d.get('label','')})\" for d in data_list])\n",
    "                else: # Pour les colonnes comme 'tokens', 'lemmas', 'pos_tags'\n",
    "                    return \" \".join(map(str, data_list))\n",
    "            \n",
    "            df_to_save[col] = df_to_save[col].apply(smart_join)\n",
    "            \n",
    "    csv_output_path = os.path.join(OUTPUT_DIR, 'corpus_linguistique_enrichi.csv')\n",
    "    try:\n",
    "        df_to_save.to_csv(csv_output_path, index=False, encoding='utf-8')\n",
    "        print(f\"\\nDataFrame enrichi sauvegardé dans : {csv_output_path}\")\n",
    "    except Exception as e_save_csv:\n",
    "        print(f\"Erreur lors de la sauvegarde du CSV: {e_save_csv}\")\n",
    "else:\n",
    "    print(\"DataFrame df_turns vide, aucune sauvegarde effectuée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63007f24",
   "metadata": {},
   "source": [
    "## 6. Conclusion et Pistes Futures pour Jean Dupont\n",
    "\n",
    "Ce notebook a présenté un pipeline complet pour le traitement et l'analyse initiale d'un corpus de transcriptions orales :\n",
    "1.  **Chargement et Nettoyage :** Les transcriptions brutes ont été lues, segmentées par locuteur et nettoyées de leurs annotations d'oralité.\n",
    "2.  **Enrichissement NLP :** Des caractéristiques linguistiques (tokens, lemmes, POS-tags, entités nommées) ont été extraites grâce à spaCy.\n",
    "3.  **Analyses Quantitatives :** Des statistiques descriptives, des fréquences de termes et de catégories grammaticales, ainsi que des visualisations (nuages de mots, histogrammes) ont été produites.\n",
    "\n",
    "**Pistes pour des analyses futures (pour Dr. Duval) :**\n",
    "* **Comparaisons statistiques :** Utiliser des tests statistiques (ex: Chi-deux, tests t) pour évaluer la significativité des différences observées entre locuteurs (fréquences de lemmes, de POS, etc.).\n",
    "* **Analyse de N-grammes :** Étudier les séquences de mots (bigrammes, trigrammes) pour identifier des collocations ou des expressions fréquentes.\n",
    "* **Topic Modeling :** Sur un corpus plus large, des techniques comme LDA (Latent Dirichlet Allocation) pourraient aider à identifier des thèmes latents dans les discours.\n",
    "* **Analyse de sentiments :** Si pertinent, évaluer la polarité (positive, négative, neutre) des propos.\n",
    "* **Exploration des Entités Nommées :** Analyser plus en détail les relations entre entités, ou leur évolution au fil des entretiens.\n",
    "* **Annotation Manuelle et Correction :** Pour des analyses très fines, une étape d'annotation manuelle ou de correction des sorties automatiques (notamment pour la segmentation ou le POS-tagging) peut être nécessaire.\n",
    "* **Intégration avec des outils d'analyse qualitative :** Les données structurées peuvent être exportées vers des logiciels d'analyse qualitative assistée par ordinateur (CAQDAS)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
