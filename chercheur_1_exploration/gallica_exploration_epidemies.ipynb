{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a51603ae",
   "metadata": {},
   "source": [
    "# Scénario 1 : Exploration des Collections de Presse de Gallica sur les Épidémies au XIXe Siècle\n",
    "\n",
    "**Chercheuse (fictive) :** Jeanne Dupont, historienne, spécialiste du XIXe siècle.\n",
    "\n",
    "**Phase du projet :** Tout début. Elle a une thématique (représentation des épidémies dans la presse française) mais besoin d'aide pour évaluer la faisabilité et constituer un premier corpus de sources primaires.\n",
    "\n",
    "**Objectif de cet accompagnement (simulé via ce notebook) :**\n",
    "1. Identifier les titres de presse pertinents et le volume d'articles/fascicules potentiels dans Gallica traitant des épidémies durant une période ciblée du XIXe siècle.\n",
    "2. Obtenir une liste structurée de ces documents avec leurs métadonnées de base (titre, périodique, date, lien Gallica).\n",
    "3. Avoir une première idée de la répartition chronologique de ces publications.\n",
    "4. Comprendre comment accéder concrètement aux documents numérisés (images, OCR si disponible) via l'API IIIF.\n",
    "\n",
    "---\n",
    "**Méthodologie :**\n",
    "* Utilisation de l'API SRU (Search/Retrieve via URL) de Gallica pour la recherche par mots-clés et filtres.\n",
    "* Ciblage d'une période spécifique du XIXe siècle pour la démonstration.\n",
    "* Extraction et structuration des métadonnées en utilisant Python et la bibliothèque Pandas.\n",
    "* Analyse exploratoire simple (comptages, distribution temporelle).\n",
    "* Introduction à l'API IIIF pour l'accès au contenu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a64133",
   "metadata": {},
   "source": [
    "## 0. Importation des bibliothèques nécessaires\n",
    "\n",
    "Avant de commencer, nous importons les bibliothèques Python qui seront utiles pour ce travail :\n",
    "* `requests` pour effectuer les requêtes HTTP vers l'API Gallica.\n",
    "* `xml.etree.ElementTree` pour parser les réponses XML de l'API SRU.\n",
    "* `pandas` pour manipuler et analyser les données structurées.\n",
    "* `matplotlib.pyplot` pour créer des visualisations graphiques.\n",
    "* `time` pour introduire des délais entre les requêtes API et ne pas surcharger les serveurs.\n",
    "* `urllib.parse` pour encoder correctement les URLs.\n",
    "* `json` pour manipuler les données JSON (notamment pour les manifestes IIIF).\n",
    "* `os` pour la gestion des dossiers de sortie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf894ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import urllib.parse\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Configuration de l'affichage pour Pandas et Matplotlib\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "plt.style.use('seaborn-v0_8-whitegrid') # Utilisation d'un style pour les graphiques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc410f41",
   "metadata": {},
   "source": [
    "## 1. Définition de la Stratégie de Recherche dans Gallica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3f29ec",
   "metadata": {},
   "source": [
    "### 1.1. Paramètres de la recherche\n",
    "\n",
    "Pour cette démonstration, nous allons cibler :\n",
    "* **Mots-clés :** Une liste de termes relatifs aux épidémies.\n",
    "* **Période :** Nous allons nous concentrer sur une période illustrative, par exemple **1830-1870**. Cette période couvre plusieurs vagues importantes de choléra en France et une activité journalistique en développement.\n",
    "* **Type de document :** Fascicules de périodiques (`dc.type all \"fascicule\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98352459",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORDS = [\"choléra\", \"épidémie\", \"contagion\", \"typhus\", \"variole\"]\n",
    "START_YEAR = 1830\n",
    "END_YEAR = 1870\n",
    "GALLICA_SRU_ENDPOINT = \"http://gallica.bnf.fr/SRU\"\n",
    "MAX_RECORDS_PER_QUERY = 50 \n",
    "MAX_TOTAL_RECORDS_TO_FETCH_PER_KEYWORD = 150 \n",
    "POLITENESS_DELAY = 1 # Secondes entre les requêtes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d059c4c5",
   "metadata": {},
   "source": [
    "### 1.2. Création des dossiers de sortie\n",
    "Si les dossiers n'existent pas, nous les créons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931c3041",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_data_folder = 'output_data'\n",
    "plots_folder = os.path.join(output_data_folder, 'plots')\n",
    "\n",
    "if not os.path.exists(output_data_folder):\n",
    "    os.makedirs(output_data_folder)\n",
    "if not os.path.exists(plots_folder):\n",
    "    os.makedirs(plots_folder)\n",
    "\n",
    "print(f\"Les données de sortie seront sauvegardées dans : '{output_data_folder}'\")\n",
    "print(f\"Les graphiques seront sauvegardés dans : '{plots_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb76b319",
   "metadata": {},
   "source": [
    "## 2. Interaction avec l'API SRU de Gallica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3adf381",
   "metadata": {},
   "source": [
    "### 2.1. Fonction pour interroger l'API SRU\n",
    "\n",
    "Nous allons créer une fonction qui construit la requête CQL (Contextual Query Language) et interroge l'API Gallica SRU.\n",
    "La fonction gérera la pagination pour récupérer plusieurs lots de résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f61e62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_gallica_sru(keyword, start_year, end_year, start_record=1, max_records=MAX_RECORDS_PER_QUERY):\n",
    "    \"\"\"\n",
    "    Interroge l'API SRU de Gallica pour un mot-clé donné et une période.\n",
    "    Retourne le contenu XML de la réponse.\n",
    "    \"\"\"\n",
    "    # Construction de la requête CQL\n",
    "    # Note: gallica all \"[keyword]\" recherche le mot-clé dans tous les champs indexés par Gallica.\n",
    "    # ocr.text all \"[keyword]\" chercherait uniquement dans le texte OCR, mais peut être plus lent/moins complet.\n",
    "    query = f'gallica all \"{keyword}\" and dc.type all \"fascicule\" and (gallicapublication_date >= \"{start_year}\" and gallicapublication_date <= \"{end_year}\")'\n",
    "    \n",
    "    params = {\n",
    "        'operation': 'searchRetrieve',\n",
    "        'version': '1.2',\n",
    "        'query': query,\n",
    "        'maximumRecords': max_records,\n",
    "        'startRecord': start_record,\n",
    "        'collapsing': 'false' # Pour éviter le regroupement des résultats par pertinence (ex: par titre de périodique)\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(GALLICA_SRU_ENDPOINT, params=params, timeout=30) # Timeout de 30s\n",
    "        response.raise_for_status()  # Lève une exception pour les codes d'erreur HTTP 4xx/5xx\n",
    "        return response.content\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erreur de requête pour '{keyword}', start_record {start_record}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aae2db5",
   "metadata": {},
   "source": [
    "### 2.2. Test rapide de la fonction `Workspace_gallica_sru`\n",
    "Testons avec un mot-clé et peu de résultats pour vérifier la connexion et le format de la réponse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9209ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_xml_content = fetch_gallica_sru(\"choléra\", 1832, 1832, max_records=5)\n",
    "if test_xml_content:\n",
    "    print(\"Réponse XML reçue (premiers 500 caractères) :\\n\", test_xml_content[:500].decode('utf-8'))\n",
    "else:\n",
    "    print(\"Aucune réponse reçue du test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4b1413",
   "metadata": {},
   "source": [
    "## 3. Parsing XML et Extraction des Métadonnées"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162b2e10",
   "metadata": {},
   "source": [
    "### 3.1. Fonctions pour parser le XML et extraire les informations\n",
    "\n",
    "L'API SRU retourne du XML qui utilise des éléments Dublin Core (préfixe `dc:`).\n",
    "Nous devons aussi gérer les namespaces XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7781316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Namespaces couramment utilisés dans les réponses SRU de Gallica\n",
    "NAMESPACES = {\n",
    "    'sru': 'http://www.loc.gov/zing/srw/',\n",
    "    'g': 'http://www.loc.gov/zing/srw/', # Parfois 'g' est utilisé comme alias pour sru\n",
    "    'dc': 'http://purl.org/dc/elements/1.1/',\n",
    "    'oai_dc': 'http://www.openarchives.org/OAI/2.0/oai_dc/',\n",
    "    'gallica': 'http://gallica.bnf.fr/ns/gallica#',\n",
    "    'xlink': 'http://www.w3.org/1999/xlink'\n",
    "}\n",
    "\n",
    "def parse_sru_response(xml_content, keyword_searched):\n",
    "    \"\"\"\n",
    "    Parse le contenu XML d'une réponse SRU et extrait les métadonnées des documents.\n",
    "    Retourne une liste de dictionnaires, chaque dictionnaire représentant un document.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    if not xml_content:\n",
    "        return records, 0 # Retourne aussi 0 pour total_results\n",
    "\n",
    "    try:\n",
    "        root = ET.fromstring(xml_content)\n",
    "        \n",
    "        # Nombre total de résultats estimé par Gallica \n",
    "        # S'il n'y a pas de résultat, la balise 'numberOfRecords' peut manquer\n",
    "        num_results_element = root.find('.//sru:numberOfRecords', NAMESPACES)\n",
    "        if num_results_element is None: # Essayer avec 'g' si 'sru' ne marche pas\n",
    "             num_results_element = root.find('.//g:numberOfRecords', NAMESPACES)\n",
    "\n",
    "        total_results = int(num_results_element.text) if num_results_element is not None and num_results_element.text.isdigit() else 0\n",
    "        \n",
    "        for record_element in root.findall('.//sru:recordData/oai_dc:dc', NAMESPACES):\n",
    "            data = {}\n",
    "            data['keyword_searched'] = keyword_searched # Ajout du mot-clé ayant permis de trouver le document\n",
    "\n",
    "            # Extraction des champs Dublin Core. On prend le premier élément trouvé pour chaque champ.\n",
    "            # Certains champs peuvent être multiples, mais pour cette exploration on simplifie.\n",
    "            for field in ['title', 'creator', 'publisher', 'date', 'type', 'format', 'language', 'source', 'relation', 'description']:\n",
    "                element = record_element.find(f'dc:{field}', NAMESPACES)\n",
    "                data[f'dc_{field}'] = element.text.strip() if element is not None and element.text else None\n",
    "            \n",
    "            # L'identifiant ARK est important\n",
    "            identifier_elements = record_element.findall('dc:identifier', NAMESPACES)\n",
    "            ark_identifier = None\n",
    "            for elem in identifier_elements:\n",
    "                if elem.text and 'ark:/' in elem.text:\n",
    "                    ark_identifier = elem.text.strip()\n",
    "                    break # On prend le premier ARK trouvé\n",
    "            data['ark'] = ark_identifier\n",
    "            data['gallica_url'] = ark_identifier # L'ARK est aussi l'URL Gallica\n",
    "\n",
    "            records.append(data)\n",
    "            \n",
    "        return records, total_results\n",
    "    \n",
    "    except ET.ParseError as e:\n",
    "        print(f\"Erreur de parsing XML: {e}\")\n",
    "        # print(\"Contenu XML problématique (premiers 1000 caractères): \", xml_content[:1000])\n",
    "        return [], 0\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur inattendue lors du parsing: {e}\")\n",
    "        return [], 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e2d853",
   "metadata": {},
   "source": [
    "### 3.2. Collecte des données pour tous les mots-clés\n",
    "\n",
    "Nous allons maintenant boucler sur nos mots-clés, interroger Gallica, et stocker toutes les métadonnées.\n",
    "Pour limiter la durée de cette démonstration, nous allons chercher au maximum `MAX_TOTAL_RECORDS_TO_FETCH_PER_KEYWORD` par mot-clé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98faf81b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_metadata = []\n",
    "total_gallica_estimate = {} # Estimation du nombre total de résultats par Gallica pour chaque mot-clé\n",
    "\n",
    "print(f\"Début de la collecte des métadonnées depuis Gallica ({START_YEAR}-{END_YEAR})...\\n\" + \"=\"*30)\n",
    "\n",
    "for keyword in KEYWORDS:\n",
    "    print(f\"\\nRecherche pour le mot-clé : '{keyword}'\")\n",
    "    keyword_metadata = []\n",
    "    current_start_record = 1\n",
    "    fetched_for_keyword = 0\n",
    "    estimated_total_for_keyword = 0\n",
    "    \n",
    "    # Récupérer la première page pour connaître le nombre total estimé de résultats\n",
    "    xml_content_first_page = fetch_gallica_sru(keyword, START_YEAR, END_YEAR, start_record=1, max_records=1)\n",
    "    if xml_content_first_page:\n",
    "        _, estimated_total_for_keyword = parse_sru_response(xml_content_first_page, keyword)\n",
    "        total_gallica_estimate[keyword] = estimated_total_for_keyword\n",
    "        print(f\"Gallica estime {estimated_total_for_keyword} résultats pour '{keyword}'.\")\n",
    "        if estimated_total_for_keyword == 0:\n",
    "            print(f\"Aucun résultat pour '{keyword}'. Passage au suivant.\")\n",
    "            time.sleep(POLITENESS_DELAY) # Pause même si pas de résultat\n",
    "            continue\n",
    "    else:\n",
    "        print(f\"Impossible de récupérer l'estimation pour '{keyword}'. Passage au suivant.\")\n",
    "        time.sleep(POLITENESS_DELAY)\n",
    "        continue\n",
    "\n",
    "    while fetched_for_keyword < MAX_TOTAL_RECORDS_TO_FETCH_PER_KEYWORD:\n",
    "        print(f\"  Récupération des résultats {current_start_record} à {current_start_record + MAX_RECORDS_PER_QUERY -1 }...\")\n",
    "        xml_content = fetch_gallica_sru(keyword, START_YEAR, END_YEAR, start_record=current_start_record)\n",
    "        \n",
    "        if xml_content:\n",
    "            new_records, _ = parse_sru_response(xml_content, keyword) # Le total ici est celui de la requête, pas global\n",
    "            if not new_records: # Plus de résultats ou erreur\n",
    "                print(f\"  Plus de résultats ou erreur pour '{keyword}' à partir de {current_start_record}.\")\n",
    "                break\n",
    "            \n",
    "            keyword_metadata.extend(new_records)\n",
    "            fetched_for_keyword += len(new_records)\n",
    "            current_start_record += MAX_RECORDS_PER_QUERY\n",
    "\n",
    "            # Si on a dépassé le nombre total estimé de résultats ou si on a récupéré moins de résultats que demandé (fin des résultats)\n",
    "            if current_start_record > estimated_total_for_keyword or len(new_records) < MAX_RECORDS_PER_QUERY :\n",
    "                 print(f\"  Fin des résultats pour '{keyword}' (ou limite estimée atteinte).\")\n",
    "                 break\n",
    "        else:\n",
    "            print(f\"  Erreur lors de la récupération pour '{keyword}', arrêt pour ce mot-clé.\")\n",
    "            break\n",
    "            \n",
    "        time.sleep(POLITENESS_DELAY) # Soyons polis avec l'API\n",
    "    \n",
    "    all_metadata.extend(keyword_metadata)\n",
    "    print(f\"Total de {len(keyword_metadata)} notices récupérées pour '{keyword}'.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*30 + \"\\nCollecte terminée.\")\n",
    "print(f\"Nombre total de notices (brutes, potentiellement avec doublons) : {len(all_metadata)}\")\n",
    "print(\"Estimations de Gallica pour le nombre total de résultats :\")\n",
    "for kw, count in total_gallica_estimate.items():\n",
    "    print(f\"  - '{kw}': {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfb3917",
   "metadata": {},
   "source": [
    "## 4. Structuration des Données avec Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1193ecc",
   "metadata": {},
   "source": [
    "### 4.1. Création du DataFrame\n",
    "\n",
    "Convertissons la liste de dictionnaires en DataFrame Pandas pour une manipulation plus aisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eedacd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_gallica = pd.DataFrame(all_metadata)\n",
    "\n",
    "print(f\"\\nLe DataFrame contient {df_gallica.shape[0]} lignes et {df_gallica.shape[1]} colonnes.\")\n",
    "print(\"Aperçu des premières lignes du DataFrame :\")\n",
    "display(df_gallica.head())\n",
    "print(\"\\nInformations sur le DataFrame :\")\n",
    "df_gallica.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f469990",
   "metadata": {},
   "source": [
    "### 4.2. Nettoyage et Prétraitement\n",
    "\n",
    "* **Doublons :** Un même document (ARK) peut être retourné pour plusieurs mots-clés. Nous allons les dédoublonner.\n",
    "* **Dates :** Le champ `dc_date` peut contenir des formats variés. Nous allons essayer de l'uniformiser et d'extraire l'année.\n",
    "* **Titre du périodique :** Souvent dans `dc_source` ou `dc_relation`. Nous allons tenter de le normaliser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc30871d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dédoublonnage basé sur l'ARK\n",
    "if 'ark' in df_gallica.columns and not df_gallica['ark'].isnull().all():\n",
    "    print(f\"\\nNombre de lignes avant dédoublonnage : {len(df_gallica)}\")\n",
    "    # Pour garder une trace des mots-clés qui ont mené à un document, on peut les agréger\n",
    "    # Cependant, pour cette première exploration, on garde juste la première occurrence\n",
    "    df_gallica_unique = df_gallica.drop_duplicates(subset=['ark'], keep='first').copy() # .copy() pour éviter SettingWithCopyWarning\n",
    "    print(f\"Nombre de lignes après dédoublonnage sur 'ark' : {len(df_gallica_unique)}\")\n",
    "else:\n",
    "    print(\"\\nColonne 'ark' non trouvée ou vide, impossible de dédoublonner.\")\n",
    "    df_gallica_unique = df_gallica.copy()\n",
    "\n",
    "# Traitement des dates\n",
    "# dc_date peut être une année, une date complète (YYYY-MM-DD), ou une période (YYYY-YYYY)\n",
    "# Pour simplifier, nous extrayons l'année. On prend les 4 premiers chiffres s'ils ressemblent à une année.\n",
    "def extract_year(date_str):\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "    # Essayer de matcher une année (4 chiffres) au début de la chaîne\n",
    "    import re\n",
    "    match = re.match(r'^(\\d{4})', str(date_str))\n",
    "    if match:\n",
    "        year = int(match.group(1))\n",
    "        # Vérifier si l'année est dans une plage plausible (ex: 1500-2025)\n",
    "        if START_YEAR <= year <= END_YEAR + 20 : # un peu de marge\n",
    "            return year\n",
    "    return None\n",
    "\n",
    "if 'dc_date' in df_gallica_unique.columns:\n",
    "    df_gallica_unique['year'] = df_gallica_unique['dc_date'].apply(extract_year)\n",
    "    # Filtrer les années qui ne sont pas dans notre période d'étude après extraction\n",
    "    df_gallica_unique = df_gallica_unique[df_gallica_unique['year'].between(START_YEAR, END_YEAR, inclusive='both')]\n",
    "    print(f\"\\nNombre de lignes après filtrage par année ({START_YEAR}-{END_YEAR}) : {len(df_gallica_unique)}\")\n",
    "else:\n",
    "    print(\"\\nColonne 'dc_date' non trouvée pour l'extraction de l'année.\")\n",
    "    df_gallica_unique['year'] = None # Créer la colonne pour éviter les erreurs futures\n",
    "\n",
    "# Titre du périodique (souvent dans dc:source ou dc:relation)\n",
    "# Exemple : \"Le Charivari. 26 septembre 1848\" -> \"Le Charivari\"\n",
    "# Pour une analyse plus poussée, il faudrait une regex plus fine ou un lien vers une notice de périodique.\n",
    "# Pour l'instant, on utilise dc:source si disponible, sinon dc:relation. dc:creator est aussi une piste.\n",
    "def extract_journal_title(row):\n",
    "    if pd.notna(row.get('dc_source')): # dc:source est souvent le titre du périodique\n",
    "        # Tenter de nettoyer un peu, ex: enlever la date si présente\n",
    "        # Ceci est très basique, pour une vraie analyse il faudrait des listes d'autorité ou des regex plus robustes\n",
    "        title_candidate = str(row['dc_source']).split('.')[0].split('(')[0].strip()\n",
    "        if len(title_candidate) > 3 : # Eviter les abréviations trop courtes\n",
    "             return title_candidate\n",
    "    if pd.notna(row.get('dc_relation')): # dc:relation peut aussi contenir le titre\n",
    "         title_candidate = str(row['dc_relation']).split('.')[0].split('(')[0].strip()\n",
    "         if len(title_candidate) > 3 :\n",
    "             return title_candidate\n",
    "    if pd.notna(row.get('dc_creator')): # Parfois le créateur est le journal\n",
    "         title_candidate = str(row['dc_creator']).split('.')[0].split('(')[0].strip()\n",
    "         if len(title_candidate) > 3 :\n",
    "             return title_candidate\n",
    "    return \"Non identifié\"\n",
    "\n",
    "if 'dc_source' in df_gallica_unique.columns:\n",
    "    df_gallica_unique['journal_title'] = df_gallica_unique.apply(extract_journal_title, axis=1)\n",
    "else:\n",
    "    df_gallica_unique['journal_title'] = \"Non identifié\"\n",
    "\n",
    "\n",
    "print(\"\\nAperçu du DataFrame nettoyé :\")\n",
    "display(df_gallica_unique[['ark', 'dc_title', 'journal_title', 'year', 'keyword_searched', 'gallica_url']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedefef2",
   "metadata": {},
   "source": [
    "### 4.3. Sauvegarde des métadonnées structurées\n",
    "\n",
    "Sauvegardons ce DataFrame nettoyé en CSV pour une réutilisation future ou pour la chercheuse.\n",
    "Nous sauvegardons un échantillon pour ne pas surcharger le dépôt Git de démonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aa276d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_size_csv = min(500, len(df_gallica_unique)) # Sauvegarder au max 500 lignes\n",
    "df_gallica_sample_csv = df_gallica_unique.sample(n=sample_size_csv, random_state=42) if len(df_gallica_unique) > sample_size_csv else df_gallica_unique\n",
    "\n",
    "csv_path = os.path.join(output_data_folder, f'gallica_epidemies_{START_YEAR}-{END_YEAR}_metadata_sample.csv')\n",
    "df_gallica_sample_csv.to_csv(csv_path, index=False)\n",
    "print(f\"\\nUn échantillon de {len(df_gallica_sample_csv)} métadonnées nettoyées a été sauvegardé dans : {csv_path}\")\n",
    "\n",
    "# Si on voulait sauvegarder tout le dataframe (attention à la taille pour un repo Git)\n",
    "# full_csv_path = os.path.join(output_data_folder, f'gallica_epidemies_{START_YEAR}-{END_YEAR}_metadata_FULL.csv')\n",
    "# df_gallica_unique.to_csv(full_csv_path, index=False)\n",
    "# print(f\"Toutes les {len(df_gallica_unique)} métadonnées nettoyées ont été sauvegardées dans : {full_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d299d8f8",
   "metadata": {},
   "source": [
    "## 5. Analyse Exploratoire Initiale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529ef9ba",
   "metadata": {},
   "source": [
    "### 5.1. Statistiques descriptives\n",
    "\n",
    "Quelques chiffres clés pour donner une première idée du corpus potentiel à Dr. Moreau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ff5459",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_gallica_unique.empty:\n",
    "    total_unique_docs = len(df_gallica_unique)\n",
    "    total_unique_journals = df_gallica_unique['journal_title'].nunique()\n",
    "    \n",
    "    print(f\"\\nStatistiques descriptives du corpus potentiel ({START_YEAR}-{END_YEAR}) :\")\n",
    "    print(f\"  - Nombre total de fascicules uniques identifiés : {total_unique_docs}\")\n",
    "    print(f\"  - Nombre de titres de périodiques uniques : {total_unique_journals}\")\n",
    "\n",
    "    if total_unique_journals > 0 and total_unique_journals != 1 and \"Non identifié\" in df_gallica_unique['journal_title'].value_counts().index: # Eviter l'erreur si 'Non identifié' est le seul.\n",
    "      print(f\"  - Nombre de fascicules avec titre de périodique 'Non identifié': {df_gallica_unique['journal_title'].value_counts()['Non identifié']}\")\n",
    "    \n",
    "    print(\"\\nTop 10 des périodiques les plus fréquents (hors 'Non identifié') :\")\n",
    "    top_journals = df_gallica_unique[df_gallica_unique['journal_title'] != \"Non identifié\"]['journal_title'].value_counts().nlargest(10)\n",
    "    display(top_journals)\n",
    "else:\n",
    "    print(\"\\nLe DataFrame est vide. Aucune analyse statistique possible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b416f120",
   "metadata": {},
   "source": [
    "### 5.2. Analyse temporelle\n",
    "\n",
    "Visualisons la distribution du nombre de fascicules par année."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b567673",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_gallica_unique.empty and 'year' in df_gallica_unique.columns and df_gallica_unique['year'].notna().any():\n",
    "    docs_per_year = df_gallica_unique['year'].value_counts().sort_index()\n",
    "    \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    docs_per_year.plot(kind='bar', color='skyblue')\n",
    "    plt.title(f'Nombre de fascicules relatifs aux épidémies par année ({START_YEAR}-{END_YEAR}) dans Gallica', fontsize=15)\n",
    "    plt.xlabel('Année', fontsize=12)\n",
    "    plt.ylabel('Nombre de fascicules', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout() # Ajuste automatiquement les paramètres pour donner un bon agencement\n",
    "    \n",
    "    # Sauvegarde du graphique\n",
    "    plot_path = os.path.join(plots_folder, f'distribution_temporelle_{START_YEAR}-{END_YEAR}.png')\n",
    "    plt.savefig(plot_path)\n",
    "    print(f\"\\nGraphique de la distribution temporelle sauvegardé dans : {plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nPas de données d'année valides pour générer la distribution temporelle.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fd0840",
   "metadata": {},
   "source": [
    "### 5.3. Interprétation (pour Dr. Moreau)\n",
    "\n",
    "* Le volume de documents potentiels semble [**important/modéré/faible** - à remplir en fonction des résultats réels].\n",
    "* La distribution temporelle montre des pics d'intérêt pour le sujet des épidémies autour des années [**années des pics**], ce qui correspond plausiblement à [**événements historiques connus, ex: vagues de choléra**].\n",
    "* Les périodiques comme [**Noms des top journaux**] semblent être des sources particulièrement riches.\n",
    "* Il est important de noter que la recherche par mots-clés a ses limites (qualité de l'OCR, polysémie des termes). Une exploration plus fine, titre par titre, ou avec des techniques de TAL plus avancées, serait nécessaire pour affiner le corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc29055a",
   "metadata": {},
   "source": [
    "## 6. Pistes pour l'Accès au Contenu via l'API IIIF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64cc210",
   "metadata": {},
   "source": [
    "### 6.1. Introduction à IIIF (International Image Interoperability Framework)\n",
    "\n",
    "IIIF est un ensemble de standards qui permet aux bibliothèques, archives et musées de diffuser des images et des contenus audiovisuels de manière standardisée et interopérable. Gallica utilise IIIF.\n",
    "\n",
    "Pour chaque document (ARK), on peut généralement accéder à un \"manifeste IIIF\". C'est un fichier JSON qui décrit le document numérique (sa structure, ses pages, les liens vers les images, parfois vers l'OCR, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a2f57d",
   "metadata": {},
   "source": [
    "### 6.2. Récupération et Analyse d'un Manifeste IIIF (Exemple)\n",
    "\n",
    "Prenons un exemple d'ARK de notre corpus pour voir comment récupérer son manifeste IIIF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e460b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_gallica_unique.empty and 'ark' in df_gallica_unique.columns:\n",
    "    # Sélectionner un échantillon avec un ARK valide\n",
    "    sample_doc_for_iiif_df = df_gallica_unique[df_gallica_unique['ark'].notna()]\n",
    "    if not sample_doc_for_iiif_df.empty:\n",
    "        sample_doc_for_iiif = sample_doc_for_iiif_df.sample(1).iloc[0]\n",
    "        ark_example = sample_doc_for_iiif['ark']\n",
    "        \n",
    "        # L'ARK est de la forme \"ark:/12148/btv1b10500001g\". Pour l'API IIIF, on enlève le \"ark:/\".\n",
    "        ark_id_for_iiif = ark_example.split('/')[-1] # Prend la partie après le dernier '/'\n",
    "        \n",
    "        manifest_url = f\"https://gallica.bnf.fr/iiif/ark:/12148/{ark_id_for_iiif}/manifest.json\"\n",
    "        print(f\"\\nExemple de récupération du manifeste IIIF pour l'ARK : {ark_example}\")\n",
    "        print(f\"URL du manifeste : {manifest_url}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(manifest_url, timeout=15)\n",
    "            response.raise_for_status()\n",
    "            manifest_data = response.json()\n",
    "            \n",
    "            # Sauvegarde du manifeste exemple\n",
    "            manifest_filename = os.path.join(output_data_folder, f'sample_iiif_manifest_{ark_id_for_iiif}.json')\n",
    "            with open(manifest_filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(manifest_data, f, ensure_ascii=False, indent=4)\n",
    "            print(f\"Manifeste sauvegardé dans : {manifest_filename}\")\n",
    "            \n",
    "            # Afficher quelques informations clés du manifeste\n",
    "            print(f\"\\nLabel du manifeste : {manifest_data.get('label')}\")\n",
    "            \n",
    "            if 'sequences' in manifest_data and len(manifest_data['sequences']) > 0:\n",
    "                num_canvases = len(manifest_data['sequences'][0].get('canvases', []))\n",
    "                print(f\"Nombre de 'canvases' (pages/vues) dans la première séquence : {num_canvases}\")\n",
    "\n",
    "                if num_canvases > 0:\n",
    "                    first_canvas = manifest_data['sequences'][0]['canvases'][0]\n",
    "                    if 'images' in first_canvas and len(first_canvas['images']) > 0:\n",
    "                        first_image_url = first_canvas['images'][0]['resource']['@id']\n",
    "                        print(f\"URL de la première image (qualité par défaut) : {first_image_url}\")\n",
    "\n",
    "            # Chercher les liens vers l'OCR (souvent en ALTO XML via 'seeAlso' ou 'rendering')\n",
    "            ocr_links = []\n",
    "            if 'seeAlso' in manifest_data:\n",
    "                for item in manifest_data['seeAlso']:\n",
    "                    if isinstance(item, dict) and item.get('format') == 'application/xml+alto':\n",
    "                        ocr_links.append(item.get('@id'))\n",
    "            # Parfois dans 'rendering' au niveau de la séquence ou du canvas\n",
    "            if 'sequences' in manifest_data and manifest_data['sequences']:\n",
    "                for rendering_item in manifest_data['sequences'][0].get('rendering', []):\n",
    "                     if isinstance(rendering_item, dict) and \"alto\" in rendering_item.get('format', \"\").lower():\n",
    "                          ocr_links.append(rendering_item.get('@id'))\n",
    "\n",
    "            if ocr_links:\n",
    "                print(f\"Liens potentiels vers l'OCR (ALTO XML) trouvés : {ocr_links}\")\n",
    "            else:\n",
    "                print(\"Aucun lien direct vers un fichier OCR ALTO trouvé dans les sections 'seeAlso' ou 'rendering' de ce manifeste.\")\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Erreur lors de la récupération du manifeste IIIF : {e}\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Erreur lors du décodage du JSON du manifeste.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur inattendue avec le manifeste IIIF : {e}\")\n",
    "    else:\n",
    "        print(\"\\nAucun document avec ARK valide trouvé dans l'échantillon pour l'exemple IIIF.\")\n",
    "else:\n",
    "    print(\"\\nDataFrame vide ou sans ARK, impossible de récupérer un exemple de manifeste IIIF.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f89e67",
   "metadata": {},
   "source": [
    "### 6.3. Conclusion pour Jeanne Dupont (Accès au contenu)\n",
    "\n",
    "L'API IIIF permet donc d'accéder :\n",
    "* Aux **images** des pages du document (avec la possibilité de spécifier la taille, la rotation, la région, etc.).\n",
    "* Aux **fichiers OCR** (quand ils existent et sont référencés), souvent au format ALTO XML. Ces fichiers contiennent le texte reconnu et la position des mots sur la page. Leur traitement permettrait des analyses textuelles plus poussées (ce qui pourrait être l'objet d'un accompagnement ultérieur)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a758f8",
   "metadata": {},
   "source": [
    "## 7. Synthèse et Prochaines Étapes (pour Jeanne Dupont)\n",
    "\n",
    "Ce premier travail d'exploration a permis de :\n",
    "1.  Confirmer la présence de nombreuses sources potentielles dans Gallica pour votre étude sur les épidémies dans la presse du XIXe siècle (durant la période {START_YEAR}-{END_YEAR}).\n",
    "2.  Fournir une première liste de métadonnées (échantillon dans `output_data/gallica_epidemies_{START_YEAR}-{END_YEAR}_metadata_sample.csv`) qui pourra servir de base à l'affinage du corpus.\n",
    "3.  Identifier des pics d'activité rédactionnelle et des titres de presse clés.\n",
    "4.  Montrer comment accéder aux documents numérisés eux-mêmes via IIIF, ouvrant la voie à une consultation directe ou à des traitements automatisés de l'OCR.\n",
    "\n",
    "**Prochaines étapes suggérées pour votre recherche :**\n",
    "* **Affiner la sélection des mots-clés :** Tester des synonymes, des expressions plus spécifiques, ou des noms de maladies moins courants.\n",
    "* **Explorer les titres de presse identifiés :** Parcourir systématiquement certains numéros des journaux les plus prolifiques.\n",
    "* **Considérer une période plus large ou différente** si nécessaire, en adaptant les paramètres de ce notebook.\n",
    "* **Récupération et analyse de l'OCR :** Si l'analyse textuelle est un objectif, un travail spécifique sur la récupération et le traitement des fichiers ALTO XML sera nécessaire.\n",
    "* **Vérification manuelle :** Toujours croiser les résultats automatiques avec une vérification manuelle d'échantillons pour évaluer la pertinence réelle des documents.\n",
    "\n",
    "N'hésitez pas si vous avez d'autres questions ou si vous souhaitez approfondir certains aspects !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abf178f",
   "metadata": {},
   "source": [
    "---\n",
    "**Considérations sur la réutilisation des données de Gallica :**\n",
    "\n",
    "Les contenus de Gallica sont, pour la plupart, dans le domaine public ou sous des licences permettant une large réutilisation, notamment pour la recherche.\n",
    "Il est toutefois recommandé de consulter les conditions d'utilisation spécifiques sur le site de Gallica et de la BnF.\n",
    "Pour un usage intensif des API, la BnF propose un portail dédié `api.bnf.fr` avec des clés d'accès qui peuvent être nécessaires pour éviter des limitations.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
